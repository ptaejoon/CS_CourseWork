{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"DM_HW5_20151550.ipynb","provenance":[{"file_id":"1VyTbgFiiVAcNeVMyn9ZAV_wE0HVTpkJc","timestamp":1591330032073}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"XVZKmC8TxLPX","colab_type":"code","colab":{}},"source":["import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tr9PZaoxLPb","colab_type":"code","colab":{}},"source":["class DDPG_Mu(nn.Module):\n","    def __init__(self):\n","        super(DDPG_Mu, self).__init__()\n","        self.fc1 = nn.Linear(3, 512)\n","        self.fc_mu = nn.Linear(512, 1)\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        mu = torch.tanh(self.fc_mu(x))*2\n","        return mu\n","    \n","    def train(self, loss):\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        \n","class DDPG_Q(nn.Module):\n","    def __init__(self):\n","        super(DDPG_Q, self).__init__()\n","        self.fc_a = nn.Linear(1, 64)\n","        self.fc_s = nn.Linear(3, 64)\n","        self.fc_1 = nn.Linear(128, 128)\n","        self.fc_q = nn.Linear(128, 1)\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n","    \n","    def forward(self, x, a):\n","        x1 = F.relu(self.fc_a(a))\n","        x2 = F.relu(self.fc_s(x))\n","        x = torch.cat([x1, x2], dim=1)\n","        x = F.relu(self.fc_1(x))\n","        q = self.fc_q(x)\n","        return q\n","\n","    \n","    def train(self, loss):\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Se0f1gCTxLPe","colab_type":"code","colab":{}},"source":["env = gym.make('Pendulum-v0')\n","Q, Q_p, Mu, Mu_p = DDPG_Q(), DDPG_Q(), DDPG_Mu(), DDPG_Mu()\n","GAMMA = 0.99 #discount factor\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 30000 #replay buffer size\n","replay_buffer = [] #다른 자료구조로 바꾸어도 상관없음.(list, queue, dict 등)\n","TAU = 0.01 #soft update parameter\n","PARAMETER_NOISE_COEF = 0.0005\n","ITER = 10 #training 함수가 호출될때 학습 iteration 횟수."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LtK3GyJjxLPh","colab_type":"code","colab":{}},"source":["def training():\n","    ###############Put your code here############\n","    #전역변수 ITER 횟수만큼 batch data를 가져와 Actor(DDPG_Mu)와 Critic(DDPG_Q)\n","    #네트워크를 학습하고, soft target update를 수행하는 함수\n","    for i in range(ITER):\n","        mini_batch = make_minibatch()\n","        state = torch.tensor([t[0].numpy() for t in mini_batch], dtype=torch.float)\n","        action = torch.tensor([t[1].numpy()for t in mini_batch], dtype=torch.float).view(-1, 1)\n","        reward = torch.tensor([t[2] for t in mini_batch], dtype=torch.float).view(-1, 1)\n","        state_prev = torch.tensor([t[3].numpy() for t in mini_batch], dtype=torch.float)\n","        done = torch.tensor([t[4] for t in mini_batch],dtype=torch.float).view(-1,1)\n","        \n","        global Q_p,Q,Mu,Mu_p\n","        \n","        with torch.no_grad():\n","             target = reward + (GAMMA * Q_p(state_prev,Mu_p(state_prev)) * (1-done))\n","             \n","        evaluation = Q(state,action)\n","        Q.train(F.smooth_l1_loss(evaluation,target))\n","        \n","        Mu.train(-Q(state,Mu(state)).mean())\n","\n","        soft_target_update(Q,Q_p)\n","        soft_target_update(Mu,Mu_p)\n","        \n","    #############################################\n","    \n","def soft_target_update(model, model_p):\n","    ###############Put your code here############\n","    #기존 네트워크(Q or Mu)에서 target 네트워크(Q_p or Mu_p)로\n","    #weight를 전역변수 TAU만큼 soft update 하는 함수.\n","    #model은 기존 네트워크, model_p는 target 네트워크를 전달받는 인자이다.\n","    #training()에서 호출되는 함수이다.\n","    for m,mp in zip(model.parameters(),model_p.parameters()):\n","      mp.data.copy_(TAU*m.data + (1-TAU) * mp.data)\n","    #############################################\n","        \n","def init_target_param(model, model_p):\n","    ###############Put your code here############\n","    #학습 시작 전 target network의 weight값을 초기화 시키는 함수.\n","    #기존 네트워크(model)의 weight값과 똑같이 초기화되어야 하며, \n","    #학습 시작 전에 한번만 호출되는 함수이다.\n","    model_p.load_state_dict(model.state_dict())\n","    #############################################\n","        \n","def parameter_noise(model):\n","    with torch.no_grad():\n","        for param in model.parameters():\n","            param.add_(torch.randn(param.size()) * PARAMETER_NOISE_COEF)\n","            \n","def store_transition(s, a, r, s_prime, done):\n","    ###############Put your code here############\n","    #현재 time step의 state(s), action(a), reward(r), \n","    #next_state(s_prime), done(에피소드가 끝나면 True, 끝나지 않았을 때 False인\n","    #bool type 변수)를 입력으로 받아서 미리 선언된 \n","    #replay_buffer에 저장하는 함수이다.\n","    #단, replay_buffer에 BUFFER_SIZE만큼의 data가 들어있을 경우,\n","    #가장 먼저 들어왔던 data를 삭제한 후 추가한다.(FIFO)\n","    global replay_buffer\n","    new_data = []\n","    new_data.append(s)\n","    new_data.append(a)\n","    new_data.append((r+8.1368022)/8.1368022)\n","    new_data.append(s_prime)\n","    #print(done)\n","    if done:\n","      new_data.append([1])\n","    else:\n","      new_data.append([0])\n","    replay_buffer.append(new_data)\n","    if len(replay_buffer) > BUFFER_SIZE:\n","        del replay_buffer[0]\n","        #replay_buffer = replay_buffer[len(replay_buffer)-BUFFER_SIZE:]\n","    #print(type(new_data[0]))\n","    #############################################\n","    \n","def make_minibatch():\n","    ###############Put your code here############\n","    #replay_buffer에서 BATCH_SIZE만큼의 transition data를 \n","    #random sampling 하여 return해주는 함수.\n","    #training() 함수에서 호출되는 함수이다. \n","    #(done은 terminal state value값을 0으로 만들어 주기 위해 필요하다.)\n","    idx = np.arange(0,len(replay_buffer))\n","\n","    np.random.shuffle(idx)\n","    idx = idx[:BATCH_SIZE]\n","    shuffled = [replay_buffer[i] for i in idx]\n","    return shuffled\n","    #############################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pNzYtgbVxLPk","colab_type":"code","outputId":"2fcade6e-d43e-4d00-e73e-028f1124d414","executionInfo":{"status":"ok","timestamp":1591285483638,"user_tz":-540,"elapsed":84936,"user":{"displayName":"taejoon Park","photoUrl":"","userId":"17674493945886111311"}},"colab":{"base_uri":"https://localhost:8080/","height":550}},"source":["reward_sum = 0.0\n","reward_list = []\n","init_target_param(Mu, Mu_p)\n","init_target_param(Q, Q_p)\n","\n","for ep in range(20000):\n","    observation = env.reset()\n","    while True:\n","        state = torch.tensor(observation, dtype=torch.float)\n","        parameter_noise(Mu)\n","        action = Mu(state).detach()\n","        observation, reward, done, _ = env.step([action.item()])\n","        reward_sum += reward\n","        next_state = torch.tensor(observation, dtype=torch.float)\n","        store_transition(state, action, reward, next_state, done)   \n","        if done:\n","            break\n","            \n","    if len(replay_buffer) >= 500:\n","        training()\n","            \n","    if ep % 20 == 19:\n","        print('Episode %d'%ep,', Reward mean : %f'%(reward_sum/20.0))\n","        if reward_sum/20.0 > -200.0:\n","            break\n","        reward_sum = 0.0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode 19 , Reward mean : -1602.609720\n","Episode 39 , Reward mean : -1708.350725\n","Episode 59 , Reward mean : -1570.443458\n","Episode 79 , Reward mean : -1539.465416\n","Episode 99 , Reward mean : -1534.141949\n","Episode 119 , Reward mean : -1547.988985\n","Episode 139 , Reward mean : -1383.283302\n","Episode 159 , Reward mean : -1147.604477\n","Episode 179 , Reward mean : -1129.390469\n","Episode 199 , Reward mean : -928.448008\n","Episode 219 , Reward mean : -876.730808\n","Episode 239 , Reward mean : -730.286245\n","Episode 259 , Reward mean : -664.381433\n","Episode 279 , Reward mean : -611.206902\n","Episode 299 , Reward mean : -422.814679\n","Episode 319 , Reward mean : -369.548631\n","Episode 339 , Reward mean : -211.543494\n","Episode 359 , Reward mean : -212.000850\n","Episode 379 , Reward mean : -208.511873\n","Episode 399 , Reward mean : -242.093678\n","Episode 419 , Reward mean : -251.851300\n","Episode 439 , Reward mean : -207.447564\n","Episode 459 , Reward mean : -404.888980\n","Episode 479 , Reward mean : -222.584052\n","Episode 499 , Reward mean : -223.297200\n","Episode 519 , Reward mean : -374.183524\n","Episode 539 , Reward mean : -282.166107\n","Episode 559 , Reward mean : -280.702268\n","Episode 579 , Reward mean : -368.506845\n","Episode 599 , Reward mean : -161.537849\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iEchXHsIxSr6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}